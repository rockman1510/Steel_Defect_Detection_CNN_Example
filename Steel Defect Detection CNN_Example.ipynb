{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image, ImageEnhance\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"/Users/huylv/Desktop/Master CS Messina/Year 2/ATDA/Steel Defect Detection/Example/data/NEU-DET\"\n",
    "IMAGES_PATH = \"/Users/huylv/Desktop/Master CS Messina/Year 2/ATDA/Steel Defect Detection/Example/data/NEU-DET/IMAGES\"\n",
    "ANNOTATIONS_PATH = \"/Users/huylv/Desktop/Master CS Messina/Year 2/ATDA/Steel Defect Detection/Example/data/NEU-DET/ANNOTATIONS\"\n",
    "IMAGES_DIR = os.listdir(IMAGES_PATH)\n",
    "ANNOTATIONS_DIR = os.listdir(ANNOTATIONS_PATH)\n",
    "images_arr = []\n",
    "annotations_arr = []\n",
    "for image in IMAGES_DIR:\n",
    "    images_arr.append(IMAGES_PATH + \"/\" + image)\n",
    "for annotation in ANNOTATIONS_DIR:\n",
    "    annotations_arr.append(ANNOTATIONS_PATH + \"/\" + annotation)\n",
    "\n",
    "images_arr.sort()\n",
    "annotations_arr.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundingBox:\n",
    "    def __init__(self, _label, _xmin, _ymin, _xmax, _ymax):\n",
    "        self.label = _label\n",
    "        self.xmin = _xmin\n",
    "        self.ymin = _ymin\n",
    "        self.xmax = _xmax\n",
    "        self.ymax = _ymax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image:\n",
    "    def __init__(self, _name, _boundingBox, _height = 200, _weight = 200):\n",
    "        self.name = _name\n",
    "        self.boundingBox = _boundingBox\n",
    "        self.height = _height\n",
    "        self.weight = _weight\n",
    "        \n",
    "    def toString(self):\n",
    "        return \"name: {}\\nbox: {}\".format(self.name, self.boundingBox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadingAnnotationImage(annotation):\n",
    "    xmlFile = open(annotation)\n",
    "    tree = ET.parse(xmlFile)\n",
    "    root = tree.getroot()\n",
    "    filename = root.find('filename').text\n",
    "    label = filename.split(\"_\")[0]\n",
    "    boundBoxes = []\n",
    "    for boundingObject in root.findall('object'):\n",
    "        xmin = boundingObject.find('bndbox').find('xmin').text\n",
    "        ymin = boundingObject.find('bndbox').find('ymin').text\n",
    "        xmax = boundingObject.find('bndbox').find('xmax').text\n",
    "        ymax = boundingObject.find('bndbox').find('ymax').text\n",
    "        box = BoundingBox(label, xmin, ymin, xmax, ymax)\n",
    "        boundBoxes.append(box)\n",
    "    image = Image(filename, boundBoxes)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadingAnnotationImage_1(annotation):\n",
    "    xmlFile = open(annotation)\n",
    "    tree = ET.parse(xmlFile)\n",
    "    root = tree.getroot()\n",
    "    filename = root.find('filename').text\n",
    "    label = filename.split(\"_\")[0]\n",
    "    boundingObject = root.findall('object')[0]\n",
    "    xmin = boundingObject.find('bndbox').find('xmin').text\n",
    "    ymin = boundingObject.find('bndbox').find('ymin').text\n",
    "    xmax = boundingObject.find('bndbox').find('xmax').text\n",
    "    ymax = boundingObject.find('bndbox').find('ymax').text\n",
    "    box = BoundingBox(label, xmin, ymin, xmax, ymax)\n",
    "    image = Image(filename, box)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_bounded_arr = []\n",
    "for annotation in annotations_arr:\n",
    "    image_bounded_arr.append(loadingAnnotationImage_1(annotation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainImages.size: 1440\ttestImages.size:360\n",
      "trainLabels.size: 1440\testLabels.size:360\n",
      "trainBBoxes.size: 1440\testBBoxes.size:360\n",
      "trainPaths.size: 1440\testPaths.size:360\n",
      "[INFO] saving testing filenames...\n"
     ]
    }
   ],
   "source": [
    "data_image = []\n",
    "labels_image = []\n",
    "boxes_image = []\n",
    "for i in range(len(image_bounded_arr)):\n",
    "    image = image_bounded_arr[i]\n",
    "    box = image.boundingBox\n",
    "    xmin = float(box.xmin) / image.weight\n",
    "    ymin = float(box.ymin) / image.height\n",
    "    xmax = float(box.xmax) / image.weight\n",
    "    ymax = float(box.ymax) / image.height\n",
    "    \n",
    "    loading_image = load_img(images_arr[i], target_size = (224, 224))\n",
    "    loading_image = img_to_array(loading_image)\n",
    "    data_image.append(loading_image)\n",
    "    labels_image.append(box.label)\n",
    "    boxes_image.append((xmin, ymin, xmax, ymax))\n",
    "        \n",
    "# convert the data, class labels, bounding boxes, and image paths to\n",
    "# NumPy arrays, scaling the input pixel intensities from the range\n",
    "# [0, 255] to [0, 1]\n",
    "data_image = np.array(data_image, dtype=\"float32\") / 255.0\n",
    "labels_image = np.array(labels_image)\n",
    "boxes_image = np.array(boxes_image, dtype = \"float32\")\n",
    "images_arr = np.array(images_arr)\n",
    "\n",
    "# perform one-hot encoding on the labels\n",
    "labelBinarizer = LabelBinarizer()\n",
    "labels_image = labelBinarizer.fit_transform(labels_image)\n",
    "\n",
    "# only there are only two labels in the dataset, then we need to use\n",
    "# Keras/TensorFlow's utility function as well\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# if len(labelBinarizer.classes_) == 6:\n",
    "#     labels_image = to_categorical(labels_image)\n",
    "# print(labels_image.shape)\n",
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "split = train_test_split(data_image, labels_image, boxes_image, images_arr, test_size=0.2, random_state = 42)\n",
    "# unpack the data split\n",
    "(trainImages, testImages) = split[:2]\n",
    "(trainLabels, testLabels) = split[2:4]\n",
    "(trainBBoxes, testBBoxes) = split[4:6]\n",
    "(trainPaths, testPaths) = split[6:]\n",
    "\n",
    "print(\"trainImages.size: {}\\ttestImages.size:{}\".format(len(trainImages), len(testImages)))\n",
    "print(\"trainLabels.size: {}\\testLabels.size:{}\".format(len(trainLabels), len(testLabels)))\n",
    "print(\"trainBBoxes.size: {}\\testBBoxes.size:{}\".format(len(trainBBoxes), len(testBBoxes)))\n",
    "print(\"trainPaths.size: {}\\testPaths.size:{}\".format(len(trainPaths), len(testPaths)))\n",
    "# write the testing filenames to disk so that we can use then\n",
    "# when evaluating/testing our bounding box regressor\n",
    "print(\"[INFO] saving testing filenames...\")\n",
    "TEST_FILE_PATH = ROOT_PATH + \"/Test.txt\"\n",
    "f = open(TEST_FILE_PATH, \"w\")\n",
    "f.write(\"{}\".format(testFilenames))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_33 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, 7, 7, 512)    0           block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 25088)        0           block5_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_139 (Dense)               (None, 512)          12845568    flatten_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_136 (Dense)               (None, 128)          3211392     flatten_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 512)          0           dense_139[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_137 (Dense)               (None, 64)           8256        dense_136[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_140 (Dense)               (None, 512)          262656      dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_138 (Dense)               (None, 32)           2080        dense_137[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 512)          0           dense_140[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bounding_box (Dense)            (None, 4)            132         dense_138[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "class_label (Dense)             (None, 6)            3078        dropout_53[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 31,047,850\n",
      "Trainable params: 16,333,162\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# load the VGG16 network, ensuring the head FC layers are left off\n",
    "vgg = VGG16(weights = \"imagenet\", include_top = False, input_tensor = Input(shape = (224, 224, 3)))\n",
    "# freeze all VGG layers so they will *not* be updated during the\n",
    "# training process\n",
    "vgg.trainable = False\n",
    "# flatten the max-pooling output of VGG\n",
    "flatten = vgg.output\n",
    "flatten = Flatten()(flatten)\n",
    "# construct a fully-connected layer header to output the predicted\n",
    "# bounding box coordinates\n",
    "bboxHead = Dense(128, activation=\"relu\")(flatten)\n",
    "bboxHead = Dense(64, activation=\"relu\")(bboxHead)\n",
    "bboxHead = Dense(32, activation=\"relu\")(bboxHead)\n",
    "bboxHead = Dense(4, activation=\"sigmoid\", name=\"bounding_box\")(bboxHead)\n",
    "# construct the model we will fine-tune for bounding box regression\n",
    "# construct a second fully-connected layer head, this one to predict\n",
    "# the class label\n",
    "softmaxHead = Dense(512, activation=\"relu\")(flatten)\n",
    "softmaxHead = Dropout(0.5)(softmaxHead)\n",
    "softmaxHead = Dense(512, activation=\"relu\")(softmaxHead)\n",
    "softmaxHead = Dropout(0.5)(softmaxHead)\n",
    "softmaxHead = Dense(len(labelBinarizer.classes_), activation=\"softmax\", name=\"class_label\")(softmaxHead)\n",
    "# put together our model which accept an input image and then output\n",
    "# bounding box coordinates and a class label\n",
    "model = Model(inputs = vgg.input, outputs = (bboxHead, softmaxHead))\n",
    "\n",
    "losses = {\n",
    "    \"class_label\": \"categorical_crossentropy\",\n",
    "    \"bounding_box\": \"mean_squared_error\",\n",
    "}\n",
    "# define a dictionary that specifies the weights per loss (both the\n",
    "# class label and bounding box outputs will receive equal weight)\n",
    "lossWeights = {\n",
    "    \"class_label\": 1.0,\n",
    "    \"bounding_box\": 1.0\n",
    "}\n",
    "\n",
    "# model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# initialize the optimizer, compile the model, and show the model\n",
    "# summary\n",
    "opt = Adam(learning_rate = LEARNING_RATE)\n",
    "model.compile(loss = losses, optimizer = opt, metrics = [\"accuracy\"], loss_weights = lossWeights)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 6)\n",
      "(1440, 4)\n",
      "(360, 6)\n",
      "(360, 4)\n",
      "()\n",
      "[INFO] training model...\n",
      "Epoch 1/20\n",
      "45/45 [==============================] - 355s 8s/step - loss: 1.3154 - bounding_box_loss: 0.0487 - class_label_loss: 1.2667 - bounding_box_accuracy: 0.6812 - class_label_accuracy: 0.5153 - val_loss: 0.5745 - val_bounding_box_loss: 0.0408 - val_class_label_loss: 0.5337 - val_bounding_box_accuracy: 0.7389 - val_class_label_accuracy: 0.8861\n",
      "Epoch 2/20\n",
      " 4/45 [=>............................] - ETA: 2:55 - loss: 0.6948 - bounding_box_loss: 0.0430 - class_label_loss: 0.6518 - bounding_box_accuracy: 0.7656 - class_label_accuracy: 0.7812"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-47ad15a04de9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] training model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m H = model.fit(trainImages, trainTargets, validation_data = (testImages, testTargets),\n\u001b[0;32m---> 24\u001b[0;31m               batch_size = 32, epochs = 20, verbose = 1)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# serialize the model to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/model.h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# construct a dictionary for our target training outputs\n",
    "trainTargets = {\n",
    "    \"class_label\": trainLabels,\n",
    "    \"bounding_box\": trainBBoxes\n",
    "}\n",
    "# construct a second dictionary, this one for our target testing\n",
    "# outputs\n",
    "testTargets = {\n",
    "    \"class_label\": testLabels,\n",
    "    \"bounding_box\": testBBoxes\n",
    "}\n",
    "\n",
    "print(trainLabels.shape)\n",
    "print(trainBBoxes.shape)\n",
    "print(testLabels.shape)\n",
    "print(testBBoxes.shape)\n",
    "\n",
    "print(np.asarray(trainTargets).shape)\n",
    "\n",
    "# train the network for bounding box regression and class label\n",
    "# prediction\n",
    "print(\"[INFO] training model...\")\n",
    "H = model.fit(trainImages, trainTargets, validation_data = (testImages, testTargets),\n",
    "              batch_size = 32, epochs = 20, verbose = 1)\n",
    "# serialize the model to disk\n",
    "model.save(ROOT_PATH + \"/model.h5\", save_format=\"h5\")\n",
    "# serialize the label binarizer to disk\n",
    "print(\"[INFO] saving label binarizer...\")\n",
    "f = open(ROOT_PATH + \"/model.h5\", \"wb\")\n",
    "f.write(pickle.dumps(labelBinarizer))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 25\n",
    "BATCH_SIZE = 32\n",
    "opt = Adam(lr = LEARNING_RATE, decay = LEARNING_RATE / NUM_EPOCHS)\n",
    "\n",
    "baseModel = tf.keras.applications.ResNet152V2(weights='imagenet')\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size = (7, 7))(headModel)\n",
    "headModel = Flatten(name = \"flatten\")(headModel)\n",
    "headModel = Dense(256, activation = \"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(6, activation = \"softmax\")(headModel)\n",
    "model = Model(inputs = baseModel.input, outputs = headModel)\n",
    "\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt, metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(trainImages, trainTargets, validation_data = (testImages, testTargets), epochs = NUM_EPOCHS)\n",
    "# H = model.fit_generator(\n",
    "# \ttrainGen,\n",
    "# \tsteps_per_epoch=totalTrain // config.BS,\n",
    "# \tvalidation_data=valGen,\n",
    "# \tvalidation_steps=totalVal // config.BS,\n",
    "# \tepochs=config.NUM_EPOCHS)\n",
    "# fit(\n",
    "#     x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None,\n",
    "#     validation_split=0.0, validation_data=None, shuffle=True, class_weight=None,\n",
    "#     sample_weight=None, initial_epoch=0, steps_per_epoch=None,\n",
    "#     validation_steps=None, validation_batch_size=None, validation_freq=1,\n",
    "#     max_queue_size=10, workers=1, use_multiprocessing=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, info = tfds.load(\"tf_flowers\", as_supervised = True, with_info = True)\n",
    "dataset_size = info.splits[\"train\"].num_examples\n",
    "class_names = info.features[\"label\"].names\n",
    "n_classes = info.features[\"label\"].num_classes\n",
    "\n",
    "(train_set, valid_set, test_set), dataset_info = tfds.load(\n",
    "    'tf_flowers',\n",
    "    split=['train[:70%]', 'train[:15%]', 'train[:10%]'],\n",
    "    with_info = True,\n",
    "    as_supervised=True,\n",
    ")\n",
    "print(dataset_size)\n",
    "print(class_names)\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
